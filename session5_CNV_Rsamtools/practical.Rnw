\documentclass[a4paper]{article}
%\usepackage{fullpage}
\usepackage[margin=1.5cm,includefoot,footskip=30pt]{geometry}
\usepackage[colorlinks=true]{hyperref}

\title{BCGES short courses, session 5: Picard tools, CNV analysis, BEDtools}
\date{}
\author{Vincent Plagnol}


%%% add bit about variant filtering, hard filtering or not
%% read depth analysis?
%% tabix?

\begin{document}

\maketitle


<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'} else {results <- 'hide'; echo  <- FALSE;}
@

\tableofcontents
\clearpage


\section{A few additional ideas to consider (40 minutes)}

\subsection{Linux piping to limit input/output usage (10 minutes)}

The lines of code below cannot be run as such but are just meant to give an example of the sort of things that multiple lines of code put together can do.
In this example the only thing written to the disk is the sorted BAM file.
No other intermediate file is written to disk.

<<piping, eval = FALSE, engine = 'bash'>>=
novoalign -c 11 -o SAM -F STDFQ -f fasta1.fq fasta2.fq  -d $reference |
   samtools view - -u -S -b |
   novosort - -t /scratch0/ -c 1 -m 3G -i -o output_sorted.bam
@ 

<<piping.res, tidy = TRUE, echo = echo, results = results>>=
#for novoalign, the -c option calls for 11 cores
## the first - in the samtools calls means to use the piping mechanism, i.e. to apply samtools to what comes out of novoalign.
## the -u option in samtools outputs uncompressed BAM, because there is no reason to compress something that is just an intermediate format in this case.
## novosort also works through the piping mechanism, and uses the same - symbol to mark this.
## the -i option of novosort creates an index for the resulting BAM file.
@


\noindent
{\bf Exercise:} Understand the structure of the code and  what the options used in the code above mean (at least for samtools which is freely available).

\subsection{gzip is magic (10 minutes)}

The GNU \texttt{gzip} utility has a very nice feature.
Look at the list of commands below:
<<gzip, engine = 'bash'>>=
gzip -c ../data/fastq_files/fastq1_1.txt > file1.fq.gz
gzip -c ../data/fastq_files/fastq2_1.txt > file2.fq.gz
cat file1.fq.gz file2.fq.gz > combined_file.fq.gz
@ 

What can you say about that combined file? Do you see what feature of \texttt{gzip} I want to highlight?
{\bf Exercise:} What other approach would seem more appropriate to create combined zip files.

<<gzip2, engine = 'bash', results = results, echo = echo>>=
cat ../data/fastq_files/fastq1_1.txt ../data/fastq_files/fastq2_1.txt \
        > combined_file2.fq
gzip -f combined_file2.fq
ls -ltrh combined_file.fq.gz combined_file2.fq.gz
##oddly the properly compressed file is a tad heavier than the other.
## but it should really not happen
@ 

\subsection{Tabix indexing of position sorted files (20 minutes)}
The indexing technology used for the BAM format relies on the fact that the rows of a BAM file are indexed by chromosome and position.
The same can be said of a VCF file for example. And as a consequence the same sort of tools can be used to index these files.
A first place to look is the \texttt{tabix} \href{http://samtools.sourceforge.net/tabix.shtml}{man page}, which highlights some of the ways \texttt{tabix} can be used.


Here is a basic example on a VCF file that you created in an earlier session.
<<tabix.vcf, engine = 'bash'>>= 
bgzip  -c ../session4_multi_sample_calling_annotation/results/GATK_msamplecalling_HC.vcf \
    > results/GATK_msamplecalling_HC.vcf.gz
tabix -p vcf -s 1 -b 2 -e 2 results/GATK_msamplecalling_HC.vcf.gz

ls -ltrh results/GATK_msamplecalling_HC.vcf.gz*

tabix results/GATK_msamplecalling_HC.vcf.gz 21:43866166-43866166
@

\noindent
{\bf Exercise:} \texttt{tabix} may need to tweak some options to work with different files formats.
The \texttt{R} code below creates a large comma separated file, with chromosome and position (two chromosomes, both 1 Mb long).
Can you index the file and use \texttt{tabix} to retrieve the rows overlapping positions 1:100000-120000.

<<create.large.file, results = 'hide', tidy = TRUE>>=
nrows <- 10000
chromosomes <- c(rep('chr1', times = 50000) ,rep('chr2', 50000))
start <- c(sort(sample(x = 10^6, size = 50000, replace = FALSE)), sort(sample(x = 10^6, size = 50000, replace = FALSE)))
end <- c(sort(sample(x = 10^6, size = 50000, replace = FALSE)), sort(sample(x = 10^6, size = 50000, replace = FALSE))) + sample(x = 1000, size = 100000, replace = TRUE)
random.number <- runif(10^5)
my.data <- data.frame(start = start,
                      end = end,
                      chromosome = chromosomes,
                      very.important.number = random.number)
write.table(x = my.data, file = 'results/large_data_file.tab',
            quote = FALSE, col.names = TRUE, row.names = FALSE, sep = '\t')
@ 

<<tabix.sol, echo = echo, results = results, engine = 'bash'>>=
##Here is how I propose to to do this
bgzip -c results/large_data_file.tab > results/large_data_file.tab.gz  ##zipping
tabix -s 3 -b 1 -e 2 -S 1 results/large_data_file.tab.gz ##indexing
tabix results/large_data_file.tab.gz  chr1:100000-120000 > results/retrieved_data.tab ##retrieving
@ 
  

\noindent
{\bf Exercise:} Another useful feature of \texttt{tabix} is that it also works on the web. Let us illustrate this with a simple example.
Find the location of the latest 1KG VCF data, and retrieve all the variants located around the {\it BRCA1} gene.
<<tabix.sol2, echo = echo, results = results, engine = 'bash'>>=
filename=ALL.wgs.phase3_shapeit2_mvncall_integrated_v5b.20130502.sites.vcf.gz
##filename=ALL.autosomes.phase3_shapeit2_mvncall_integrated_v5b.20130502.sites.vcf.gz
link=ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/$filename

##note that the -f option is required here, otherwise the code complains
tabix -f $link 17:41243452-41277500 > results/1KG_BRCA1.vcf  
@ 
  
%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Some examples PICARD tools (50 minutes)}

\subsection{Marking duplicates}

Here is an example below of a typical PICARD call to mark duplicates.
Make sure that you can run some version of this script, this is a very standard process to go through.

<<dupli, eval = TRUE, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
java -Xmx4g -jar /usr/local/gatk/picard.jar MarkDuplicates  ASSUME_SORTED=true \
      REMOVE_DUPLICATES=FALSE \
      INPUT=$BAM \
      OUTPUT=results/HG00130.mapped.bam METRICS_FILE=results/HG00130.metrics.out
@ 

\noindent
{\bf Exercise:} Can you find where, in the headers of the resulting BAM file, how information is recorded about this duplicate marking step?

<<dups, echo = echo, results = results, engine = 'bash'>>=
samtools view -H results/HG00130.mapped.bam | grep MarkDupl | tail -1
@

\subsection{Collect summary statistics}

And here is some example code to compute summary statistics on a BAM file:
<<sumStats, eval = TRUE, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
java -Xmx4g -jar /usr/local/gatk/picard.jar CollectAlignmentSummaryMetrics \
    INPUT=$BAM \
    OUTPUT=results/insert_size.txt
@ 
  
Make sure that you can run that code, and look at the output files.
Check that you understand the meaning of the summary statistics.


\subsection{Convert a BAM file to fastq}

Now something quite a bit more challenging: try the following script to convert a BAM file to fastq:
Here is a first attempt

<<fastq.v1, eval = TRUE, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
java -Xmx4g -jar /usr/local/gatk/picard.jar SamToFastq \
    INPUT=$BAM \
    FASTQ=results/read1.fq SECOND_END_FASTQ=results/read2.fq
@ 

The issue is that some reads do not have a mate, and that creates issues with the FASTQ files.
We need a fix to deal with that error message.
We can try the following, which should remove non-mapped reads:

<<remove.non.mapped, eval = TRUE, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
samtools view -f 0x0001 -f 0x0002 -b -o results/only_paired.bam $BAM

java -Xmx4g -jar /usr/local/gatk/picard.jar SamToFastq INPUT=results/only_paired.bam \
            FASTQ=results/read1.fq SECOND_END_FASTQ=results/read2.fq
@ 

But it fails again. With only 2 unpaired mates this time, so things are getting better. 
We can find out what these are:

<<find.bad.reads, eval = TRUE, engine = 'bash'>>=
samtools view results/only_paired.bam | awk '{print $1}' | sort | uniq -c | sort -r | tail -5
@ 

Now let us remove these problematic reads, but we have to do it manually. 
I could not find a better way to do this than what is shown below:

<<remove.bad.reads, eval = TRUE, engine = 'bash'>>=
samtools view -h results/only_paired.bam |
       awk '{if (($1 != "SRR707198.24062323") && ($1 != "SRR707198.20187117")) print}' \
       > results/paired_fixed.sam

samtools view -b -S -o results/paired_fixed.bam results/paired_fixed.sam
@ 

The second line above takes a SAM file input (option -S) and returns a BAM file (option -b).
The resulting file can, at last, be converted into a FASTQ format:

<<fastq.v2, eval = TRUE, engine = 'bash'>>=
java -Xmx4g -jar /usr/local/gatk/picard.jar SamToFastq INPUT=results/paired_fixed.bam \
                       FASTQ=results/read1.fq SECOND_END_FASTQ=results/read2.fq
@

{\bf Exercise:} Can you find a better way to convert a BAM to fastq? I could not but I have not looked extensively.



%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{\texttt{BEDtools} (30 minutes)}

\href{http://bedtools.readthedocs.org/en/latest/}{\texttt{BEDtools}} utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks.
Individually, none of these tools looks impressive, but together with \texttt{samtools} they can be used to achieve impressive tasks.
I could simply point you to the excellent documentation of the software (and you certainly should have a look) but I will also highlight a few useful ideas.

\subsection{Computing depth of coverage for targeted DNA sequencing experiments}

Some fancy bits of code are proposed in that \href{http://gettinggeneticsdone.blogspot.co.uk/2014/03/visualize-coverage-exome-targeted-ngs-bedtools.html}{blog post}.
We will simply follow these ideas to illustrate what \texttt{BEDtools} can do.
The first step is to actually obtain a BED file that describes where the exons of a gene of interest are located.
I generated this using this \href{https://www.biostars.org/p/93011/}{very good post}, and if there is time I suggest that you look into this.
My modified scripts (which worked in this case) are located in \texttt{tables/join.sh}.

\noindent
{\bf Optional exercise:} Reproduce the steps suggested in the Biostars answer. Scripts will need to be updated slightly depending on the exact computer that you use.

This is however not the main topic of the day and feel free to simply use the outcome BED file \texttt{tables/UBASH3A.bed}.

<<coverage, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
bedtools2.24 coverage -hist -b $BAM -a tables/UBASH3A.bed | grep ^all > results/HG00130_coverage.txt
head results/HG00130_coverage.txt
@ 

What you get from this is the data underlying an histogram. 
The fourth column tells you that you are looking at a total of 2,442 bp.
For the first row, the third column tells you 284 of these positions are covered by 0 read, 14 are covered with 1 read, 35 positions with 2 reads...
This is not very readable, so the output can for example be processed in \texttt{R} using the code below:

<<process.bed.output, fig.width = 4, fig.height = 4>>=
gcov = read.table("results/HG00130_coverage.txt");

# Create a cumulative distribution from the "raw" hist
# (truncate at depth >=1000)
gcov_cumul = 1 - cumsum(gcov[,5])

## Create a plot of the CDF
plot(x = gcov[2:143,2], y = gcov_cumul[1:142],
     col='darkred', type='l', lwd=2,
     xlab="Depth",
     ylab="Fraction of capture target bases >= depth",
     ylim=c(0,1.0))
@ 

\noindent
{\bf Exercise:} Make sure you can run and understand the \texttt{R} code, and what the different columns of the output mean.
Can you rewrite the code above so that it loops over all the GBR BAM files? You will probably need the function \texttt{find} to do this.
You can also try the fancier looking \texttt{parallel} implementation proposed by Stephen Turner's \href{http://gettinggeneticsdone.blogspot.co.uk/2014/03/visualize-coverage-exome-targeted-ngs-bedtools.html}{blog post} but be careful, \texttt{parallel} is not systematically installed on all machines so thuis may not be possible.

<<bedtools.sol, engine = 'bash', echo = echo, results = results>>=
for BAM in `find ../data/BAM_files -name \*bam`; do
  code=`basename $BAM .bam`
  bedtools2.24 coverage -hist -b $BAM -a tables/UBASH3A.bed | grep ^all > results/${code}_coverage.txt
done
@ 

\subsection{Find regions that are not covered by a BAM file}

This originated from the following question, asked on Twitter:
``Given a.bam and b.regions.bed, how to get the parts of b.regions.bed that are not covered by a.bam?''
See the example below:

<<bedtools.fancy, engine = 'bash'>>=
BAM=../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
regions=tables/UBASH3A.bed
bedtools2.24 genomecov -ibam $BAM -bga \
         | awk '$4==0' | bedtools2.24 intersect -a $regions -b - > results/not_covered.bed
@ 

\noindent
{\bf Question:} Understand what the three piping steps do in this example. Look at the output. How many bases are not covered?
Is this consistent with the result file of the previous section?



%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Use \texttt{Rsamtools} to identify reads characteristic of a deletion (30 minutes)}

Another strategy to detect CNVs consists of picking up reads that show an unusual pattern.
For example, two pairs further apart than they normally should is potentially flagging a deletion.
In this exercice I proposed to look at a well described 20 kb CNV located near the {\it IRGM} gene.
We can start by looking at the location of the variant using the data from the Conrad et al paper.
These are loaded into the ExomeDepth package, so this is one way to access them.

<<IRGM, eval = TRUE, message = FALSE>>=
library(ExomeDepth)
data(Conrad.hg19)
IRGM <- Conrad.hg19.common.CNVs[ as.character(GenomicRanges::seqnames(Conrad.hg19.common.CNVs)) == "5" &
                                GenomicRanges::start(Conrad.hg19.common.CNVs) > 150200000
                                & GenomicRanges::end(Conrad.hg19.common.CNVs) < 150240000,]
write.table(x = as(IRGM, "data.frame"),
            row.names = FALSE, sep = '\t', quote = FALSE,
            file = 'IRGM_common.tab')
@ 


We now want to look for individuals that carry this variant. 
Low coverage whole genome data for the 1KG sample HG00123 will do the job.

\noindent
{\bf Exercise:} Identify the BAM file for mapped reads for HG00123 and download the slice chr5:150820438-150845438 (build hg38) using \texttt{samtools view}.
How would you filter pairs of reads that span over the deletion in this sample?


<<download, eval = FALSE, engine = 'bash', results = results, echo = echo, tidy = TRUE>>=
###filename below
fileName=data/HG00123/alignment/HG00123.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram
### now I add the address of the ftp site
BAM=ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/${fileName}
### and I use samtools to extract the reads I need
### note that the output file should be in your results folder, so you may skip this download step if it slows you down
samtools1.2 view -b $BAM chr5:150820438-150845438 > results/HG00123.bam
samtools1.2 view results/HG00123.bam  | awk '{if (($9 > 15000) && ($9 < 25000)) print}' | less -S
@

\noindent
{\bf Exercise:} Use IGV to visualize the deletion and make sure you can clearly see these reads.\\
{\bf (Optional) Exercise:} Can you write a R script (using \texttt{Rsamtools}) to identify these reads?


<<index, echo=echo, results = results, engine = 'bash'>>=
##first thing first we index the output file, because we need this for Rsamtools to do the job
samtools index results/HG00123.bam  
@
<<Rsamtools, message = FALSE, echo = echo, results = results>>=
library(Rsamtools)
which <- GRanges(seqnames=Rle("chr5"),
                 IRanges(start = 150820438, end = 150845438))

what <- c("qname", "pos", "mpos", "isize")
param <- ScanBamParam(which=which, what=what)

bam.reads <- scanBam(file = "results/HG00123.bam",
                     param=param)

bam.reads[[1]]$isize[ which( abs(bam.reads[[1]]$isize) > 20000) ]
bam.reads[[1]]$qname[ which( abs(bam.reads[[1]]$isize) > 20000) ]

@
%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Read depth analysis to detect CNVs (30 minutes)}

Another useful way to detect CNVs is read depth based analysis.
This is a non straightforward analysis, and results can vary greatly between one dataset and the next.
Hence, this part of the class is located at the end and can be skipped if there is not enough time.
Nevertheless, it is a good idea to understand what this does and how to run an analysis like this one.
To do so, look and run the example code provided in \texttt{exomeDepth\_example.R}.
Familiarise yourself with the code, modify it to run with the sample \texttt{UCLG\_94\_sorted\_unique.bam}. 
You should be able to see a two exon heterozygous deletion located in the gene {\it GATA2}, which is causal in this individual.
The code is listed below as well, but not executed on the fly as part of the document because it takes a bit long to do so.
But there should be time to discuss what this does in class.

<<exomeDepth, message = FALSE, eval= FALSE>>=
library(ExomeDepth)
@
<<exomeDepth.2, message = FALSE, eval = FALSE>>=
ExomeCount <- read.table("../data/exon_read_count.tab.gz", header = TRUE)
my.test <- ExomeCount$UCLG.186_sorted_unique.bam
all.bams <- grep(pattern = 'bam$', names(ExomeCount), value = TRUE)
my.ref.samples <- subset( all.bams, all.bams != "UCLG.186_sorted_unique.bam")

my.reference.set <- as.matrix(ExomeCount[, my.ref.samples])

my.choice <- select.reference.set (test.counts = my.test,
                                   reference.counts = my.reference.set,
                                   bin.length = (ExomeCount$end - ExomeCount$start)/1000,
                                   n.bins.reduced = 10000)

my.matrix <- as.matrix( ExomeCount[, my.choice$reference.choice, drop = FALSE])
my.reference.selected <- apply(X = my.matrix,
                               MAR = 1,
                               FUN = sum)

######### create the ExomeDepth object
all.exons <- new("ExomeDepth",
                 test = my.test,
                 reference = my.reference.selected,
                 formula = 'cbind(test, reference) ~ 1')

all.exons <- CallCNVs(x = all.exons,
                      transition.probability = 10^-4,
                      chromosome = ExomeCount$chromosome,
                      start = ExomeCount$start,
                      end = ExomeCount$end,
                      name = ExomeCount$exons)


################# Now annotate the CNV calls
data(Conrad.hg19)
all.exons <- AnnotateExtra(x = all.exons,
                           reference.annotation = Conrad.hg19.common.CNVs,
                           min.overlap = 0.5,
                           column.name = "Conrad.hg19")

data(exons.hg19)

exons.hg19.GRanges <- GRanges(seqnames = exons.hg19$chromosome,
                              IRanges(start=exons.hg19$start,end=exons.hg19$end),
                              names = exons.hg19$name)

all.exons <- AnnotateExtra(x = all.exons,
                           reference.annotation = exons.hg19.GRanges,
                           min.overlap = 0.0001,
                           column.name = "exons.hg19")



####### Take the final table, sort by significance (Bayes factor)
my.final.table <- all.exons@CNV.calls
my.final.table <- my.final.table[ order(my.final.table$BF, decreasing = TRUE),]



print(table(is.na(my.final.table$Conrad.hg19)))

print(head(my.final.table))

pdf("figure/LRBA_example.pdf")
plot (all.exons,
      sequence = "4",
      xlim = c(151749334 - 100000, 151948710 + 100000),
      count.threshold = 20,
      main = "LRBA gene",
      with.gene = TRUE)

dev.off()
@




\end{document}
