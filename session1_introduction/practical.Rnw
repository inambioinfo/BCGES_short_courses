\documentclass[a4paper]{article}
\usepackage[colorlinks=true]{hyperref}
%\usepackage{fullpage}

\usepackage[margin=1.5cm,includefoot,footskip=30pt]{geometry}

\title{BCGES short courses, session 1: Introduction, file formats, shell scripting, Galaxy}
\date{}
\author{Vincent Plagnol}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%knit('practical.Rnw'); system('pdflatex practical')

\maketitle

<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'; fig.show <- 'asis'} else {results <- 'hide'; echo  <- FALSE; fig.show <- 'hide';}
options(tidy=TRUE, width=100)
if (!file.exists("results")) dir.create("results")
@

The aim of this first session is to introduce the tools that we will use to manipulate sequence data.
Generally speaking, we will deal with either \texttt{R}, bash scripting or \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{Galaxy}.


\texttt{R} is not usually the preferred way to deal with HTS data.
One reason for this is that the general approach of \texttt{R} is to load data into the RAM as a first instance, which is not practical when the datasets are very large.
Shell scripts (aka command line tools) that read the data on a line per line basis are usually preferred.
Nevertheless, \texttt{R} has developed tools to overcome these limitations and there is, in fact, surprisingly much that one can do with it.
It is also a practical tools for teaching purposes, which is useful in the context of these short courses.
So we will use \texttt{R} mostly to play with the data and show the sort of things one may want to do with it.

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Basic fastq reading and quality control (1h)}

\subsection{Basic shell scripting to read NGS files (20 minutes)}


Many of the standard HTS formats are simple text files, with tightly defined specifications to allow effective parsing.
Some of these files can be compressed and/or indexed to enable quicker access.
The first esstential step is to be confortable with reading/compressing/uncompressing various text files.

<<head, engine='bash'>>=
head ../data/fastq_files/fastq1_1.txt
@ 


<<tail, engine='bash'>>=
tail ../data/fastq_files/fastq1_1.txt
@ 

If you need more information, shell scripts have useful manual page that can be accessed using the \texttt{man} command.
<<man, engine='bash', eval=FALSE>>=
man head
man tail
@ 

One can parse a text file using a variety of commands, and it is useful to become familiar with the following:
<<reads, engine='bash', eval = FALSE>>=
cat ../data/fastq_files/fastq1_1.txt
less ../data/fastq_files/fastq1_1.txt
more ../data/fastq_files/fastq1_1.txt
less -S ../data/fastq_files/fastq1_1.txt
@ 

\noindent
{\bf Exercise:} Can you see the differences between these different ways of reading a file?

\vspace{1.5cm}


A routine that is often useful is \texttt{wc} that counts the words/character/lines of a file. Try:
<<wc, engine='bash', eval= FALSE>>=
wc  ../data/fastq_files/fastq1_1.txt
wc  -l ../data/fastq_files/fastq1_1.txt
@


\noindent
{\bf Exercise:} Using the man page, find a way to print the first 20 lines of a fastq file (and what about the last 20 lines)?\\
{\bf Exercise:} Based on the \texttt{wc} output, how many reads do these fastq files contain?

<<wc.answers, engine='bash', eval= TRUE, echo = echo, results = results>>=
## Some answers to the two questions above:
head -20 ../data/fastq_files/fastq1_1.txt > results/first_20.txt
tail -20 ../data/fastq_files/fastq1_1.txt > results/last_20.txt
## each read uses 4 lines, so the output of wc -l must be divided by 4 to get the read count.
@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Using the shortRead package in R (15 minutes)}
We start by loading one of the most relevant library, called ``ShortReads''. This package may not be installed but it can easily done so by running:
<<install, eval = FALSE>>=
source("http://bioconductor.org/biocLite.R")
biocLite("ShortRead")
@ 

<<load, results = 'hide', message = FALSE>>=
library('ShortRead')
@ 

As a starting point it is possible to read some of the examples fastq files and create relevant \texttt{R} objects.
<<readFastq, eval=TRUE>>=
fastq1.1 <- readFastq('../data/fastq_files/fastq1_1.txt')
fastq1.2 <- readFastq('../data/fastq_files/fastq1_2.txt')
@ 

We can now display the sequences and the qualities. Note that specific classes have been defined to store each of these objects.
A lot of work has gone into figuring out how to do this.

<<showFastq>>=
reads <- sread(fastq1.1)
class(reads)
head(as.character(reads))

ids <- id(fastq1.1)
class(ids)
head(as.character(ids))
@ 

It is also possible to use this package to look at quality scores.
For example, following up on the example above:

<<showQuals>>=
quals <- quality(fastq1.1)
class(quals)
quals
@ 


The \texttt{ShortRead} package will attempt to guess what these quality scores mean, for example see:
<<guessQual>>=
encoding(quality(fastq1.1))

fastq2.1 <- readFastq('../data/fastq_files/fastq2_1.txt')
encoding(quality(fastq2.1))
@ 

\noindent
{\bf (Optional, and somewhat difficult) Exercise:} Generate a plot showing the average Phred score as a function of the position in the read.

<<plotQual, eval = TRUE, results = results, echo = echo, fig.show = fig.show, fig.width = 7, fig.height = 7>>=
## Here is how I would create that plot
ave.qual <- apply(FUN = mean, MAR = 2, as(quals, 'matrix'))
plot (x = 1:length(ave.qual),
      y = ave.qual,
      xlab = 'Position in read',
      ylab = 'Average Phred score')
@ 

  
\subsection{Using the Galaxy server (30 minutes)}

Start by identifying a working instance of the Galaxy server from this \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{location}.
There are multiple choices here, so feel free to experiment.
In case of doubt, I propose that you use the \href{https://usegalaxy.org/}{main Galaxy instance} which I know works for the purpose of this exercise.
However, you can try different options but be ready for small and subtle differences and be flexible associated with different versions of the software.

\noindent
{\bf Exercise:} Create an account on one of these Galaxy servers and upload the pair of fastq {\it fastq2\_1.txt} and {\it fastq2\_2.txt}.
Run the \texttt{fastqc} code on these files (note that a standalone version also exists and can be found \href{http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}{here}).

<<tips1, tidy = TRUE, echo = echo, results = results>>=
#A difficulty is that the two fastq files need to be joined together to mark the fact that they are paired end reads.
#There is a tool to do this called FASTQ joiner.
#And if FASTQ joiner does not work, it is probably because you need to run FASTQ Groomer first, which does some magic to ``prepare'' the fastq files.
#But these things may differ for different instances of the Galaxy server. Be flexible!
@ 

\noindent
{\bf (Optional) Exercise:} Run \texttt{fastqc} locally (after downloading it from the  \href{http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}{web}) and compare the output with what you get from Galaxy (it should be identical!).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Reading and manipulating SAM, BAM and CRAM files using \texttt{samtools} (about 1.5h)}
BAM files are compressed files that contain the information from the FASTQ files, plus additional information about the location where the reads map.
A key feature of the BAM files is that they can be indexed, i.e. an associated file contains information about where each of the reads are located in the file.
It allows very quick retrieval of reads that map to a given genomic location, which is the typical way one wants to use BAM files (for example, to extract all the reads that map to a gene of interest in order to find rare variants).
CRAM provides most of these features but in a more effective compression format.
This is the future, if we can just manage to upgrade our tools.

\texttt{samtools} is the key piece of software that is used to read, write and index BAM files.
\texttt{samtools} has been maintained essentially by Heng Li until the release of version 1.0
However, some tools used in this class still rely on the ``old'' samtools. For that reason you should find two versions of \texttt{samtools} installed.
\begin{itemize}
\item The old version which is in your path and simply called \texttt{samtools}. This one will disappear in the next iteration of the class, but it is still here for the time being.
\item The updated version 1.2 of \texttt{samtools}, for which you can find the manual \href{http://www.htslib.org/doc/samtools.html}{at this location} and can be called using \texttt{samtools1.2}. This is the version with CRAM support.
\end{itemize}

So in most cases, rather than \texttt{samtools} you probably want to use \texttt{samtools1.2}, though many functions are equivalent.

\subsection{Converting between formats (10 minutes)}

One thing one does extensively with sequence data is shifting from one format to the next.
Consider for example this BAM file, which has been provided as an example BAM for the class:
<<test.file, engine='bash'>>=
ls -ltrh ../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
@ 

We will have a go at the following \noindent
{\bf Exercise:} convert this file to CRAM and check that the compression has improved.
<<CRAM, engine = 'bash', echo = echo, results = results>>=
samtools1.2 view -C  -o is_it_better_now.cram ../data/BAM_files/HG00130.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
ls -ltrh is_it_better_now.cram
@ 



\subsection{More \texttt{samtools} (40 minutes)}

A very useful feature of \texttt{samtools} is that it can work over a ftp site, by downloading the index locally and only pulling the reads that are relevant.
That allows to access rich datasets online without having to download very large files.
The exercise below illustrates some of these features.

\noindent
{\bf Exercise:} The following should be useful as an introduction to the sort of things one may want to do with \texttt{samtools}.
The manual page should have all the commands and ideas to go through these, so best to have a go and try.

\begin{enumerate}
\item Find the CRAM file for sample HG00118 on the ftp site. Look at \href{ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/}{this location}.
\item Using \texttt{samtools view} (version 1.2 required for CRAM analysis) over the web, download the {\it BRCA1} reads for the sample \texttt{HG00118}.
  Make sure that your output is in BAM format. \\ \noindent
{\bf Note:} This step may seem difficult at first so the resulting BAM file should be available by default as part of the github set of files. 
<<view, engine = 'bash', echo = echo, results = results, tidy = TRUE>>=
## here is the location of a CRAM file on the 1,000 Genomes ftp
oneKGftp=ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/
CRAM=${oneKGftp}/data/HG00118/alignment/HG00118.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram
##Then samtools view, note the -b option to generate a BAM output
samtools1.2 view -b -o results/BRCA1_HG00118.bam $CRAM chr17:43044295-43125370
@ 
\item Index this BAM file using \texttt{samtools index}.
<<index, engine = 'bash', echo = echo, results = results>>=
##Basic index command below, a must know!
samtools1.2 index results/BRCA1_HG00118.bam
@ 
\item Still using \texttt{samtools view}, subset the BAM file for the first coding exon (not the UTR) of {\it BRCA1}  (use transcript ENST00000357654 from Ensembl) and output in SAM format.
<<exon1, engine = 'bash', echo = echo, results = results>>=
## the line of code below should create that SAM file
samtools1.2 view results/BRCA1_HG00118.bam chr17:43044295-43045802 > results/BRCA1_exon1.sam
@ 
\item Exercise: What do these lines do?
<<view.2, engine = 'bash', eval = TRUE>>=
samtools1.2 view -f 0x0002 results/BRCA1_HG00118.bam | wc -l
samtools1.2 view -F 0x0002 results/BRCA1_HG00118.bam | wc -l
samtools1.2 view -F 0x0040 results/BRCA1_HG00118.bam | wc -l
@ 
<<view.tips, engine = 'bash', eval = FALSE, results = results, echo = echo>>=
## Some short comments for each line below
samtools1.2 view -f 0x0002 results/BRCA1_HG00118.bam #prints properly mapped pairs
samtools1.2 view -F 0x0002 results/BRCA1_HG00118.bam #excludes properly mapped pairs
samtools1.2 view -F 0x0040 results/BRCA1_HG00118.bam #print first read of pair
@ 
\end{enumerate}


\subsection{Understanding the SAM/BAM headers (10 minutes)}

Key information about a BAM file can be obtained from the headers.
\texttt{Samtools} is once again the key tool to read these data.
You can view these headers using:
<<headers, engine = 'bash'>>=
samtools view -H results/BRCA1_HG00118.bam > results/headers.txt
@ 
<<headers.2, engine = 'bash', eval = FALSE>>=
less -S results/headers.txt
@ 

\noindent
{\bf Exercise:} Go through the output of \texttt{samtools1.2 view -H} and make sure you understand what all the fields mean.
This will be discussed in class. In the case of the downloaded 1KG file, much processing has happened so not all fields will make sense, but the important thing is to understand the general philosophy of the file. The ID, LB, SM tags are particularly useful.



\subsection{Using \texttt{R} and \texttt{Rsamtools} (20 minutes)}

The \texttt{Rsamtools} package in \texttt{R} is very effective to parse BAM files, and extremely memory efficient, making full used of BAM indexes.
Look at the example below for example.
Inspect the output object called \texttt{bam.reads}.
Can you understand its structure? See what it contains and how the data are organised?
We will be using these tools later on in the CNV analysis section.

<<Rsamtools>>=
library(Rsamtools)
library(GenomicRanges)

which <- GRanges(seqnames=Rle("21"),
                 IRanges(start = 43000000, end = 45000000))

what <- c("rname", "strand", "pos", "qwidth", "seq")
param <- ScanBamParam(which=which, what=what)

bam.reads <- scanBam(file = "../data/BAM_files/HG00251.mapped.ILLUMINA.bwa.GBR.exome.20121211.bam",
                     param=param)
names(bam.reads[[1]])
head(bam.reads[[1]]$pos)
@ 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Merging overlapping paired-end reads (20 minutes)}

Another useful thing to be aware of is the possibility to merge read pairs that overlap in the middle.
This happens when the combined length of both reads exceeds the length of the DNA fragment they originate from.
A review of tools to perform this task is available \href{http://thegenomefactory.blogspot.de/2012/11/tools-to-merge-overlapping-paired-end.html}{here}.
I have tested a few of these tools and I have been happy with two of them: 
\begin{itemize}
\item One is \href{http://sco.h-its.org/exelixis/web/software/pear/doc.html}{PEAR}
\item And the other is \href{http://ccb.jhu.edu/software/FLASH/}{FLASH}.
\end{itemize}

If time allows and this is something you are interested in, I propose to have a go at running \texttt{PEAR}.
Start by downloading the pre-compiled binary to your own computer (this program is not preinstalled).
You should be able to run something like the command below:

<<pear, engine = 'bash', eval = TRUE, results = results>>=
pear -e -v 10 -c 40 -f ../data/fastq_for_merging/reads1.fq \
                    -r ../data/fastq_for_merging/reads2.fq \
                    -o results/pear_merged
@

And now something very similar using \texttt{flash}:

<<flash, engine = 'bash', eval = TRUE, results = results>>=
flash -m 10 -o results/flash_merge ../data/fastq_for_merging/reads1.fq \
                                   ../data/fastq_for_merging/reads2.fq \
                                   1> results/flash.out 2> results/flash.error
@ 

Check what the output looks like, in particular the read length of the assembled reads. The example provided is meant to be an easy one, and all reads should assemble properly.
Have a look at the options that \texttt{PEAR} and \texttt{FLASH} use.
There are some discrepancies between the output of both tools.
It may be interesting to understand them and assess which you think is doing the best job.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Trimming reads (20 minutes)}

Trimming reads usually refers to the process of cutting the end of the reads, because these are more likely to be of lower quality.
One good way to illustrate the importance of trimming is to follow-up on the example above of merging paired end reads but with a more difficult example.

Here is what it looks like with \texttt{flash}:
<<failMerge, engine ='bash', eval = TRUE, results = results>>=
flash -m 10 -o results/tough_merge ../data/fastq_for_merging/tough_read1.fq \
                                   ../data/fastq_for_merging/tough_read2.fq \
                                   1> results/flash_tough.out 2> results/flash_tough.error
@ 


\noindent
{\bf Exercise:} Can you understand why the merging fails so miserably in this example? What is the source of the problem?
(it is actually quite a NextSeq specific problem).

So the conclusion of the previous exercise is that the outcome looks terrible! Now we will want to fix this.
My tool of choice is called \texttt{sickle} and it has not failed me so far.
What we want to do is cut the right end of each read (and only the right end!) as long as the average quality is lower than 20.

<<sickle, engine='bash', eval = TRUE, results = results>>=
sickle pe -t sanger -l 0 -q 20 -f ../data/fastq_for_merging/tough_read1.fq \
                                -r ../data/fastq_for_merging/tough_read2.fq \
                                -o results/tough_read1_trimmed.fq \
                                -p results/tough_read2_trimmed.fq \
                                -s no_use_because_l_is_zero.fq
@ 

\noindent
{\bf Exercise:} Can you now try to merge again using the trimmed files? How much better does it look?
Do you understand where the difference comes from? What do you think the \texttt{awk} command line below is doing.
\texttt{awk} is a very handy command line utility and I recommend getting used to it.

<<awk.guess, engine='bash', eval = TRUE, results = results>>=
awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' results/tough_read1_trimmed.fq
awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' results/tough_read2_trimmed.fq
@ 


And now you can look at the output of the \texttt{flash} code, and see how much better things appear.
Make sure you understand why that is the case.

<<goodMerge, engine ='bash', eval = TRUE, results = results, echo = echo>>=
flash -m 10 -o results/improved_merge results/tough_read1_trimmed.fq \
                                      results/tough_read2_trimmed.fq \
                                      1> results/flash_better.out 2> results/flash_better.error
@ 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section*{Session info}
<<sessionInfo>>=
sessionInfo()
@ 


\end{document}
