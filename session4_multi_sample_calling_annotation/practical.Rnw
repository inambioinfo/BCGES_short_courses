\documentclass[a4paper]{article}
\usepackage[colorlinks=true]{hyperref}
%\usepackage{fullpage}
\usepackage[margin=1.5cm,includefoot,footskip=30pt]{geometry}

\title{BCGES short courses, session 4: Multi-sample calling, VCFtools and variant annotation}
\date{}
\author{}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%knit('practical.Rnw'); system('pdflatex practical')

\maketitle


<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'; fig.show <- 'asis'} else {results <- 'hide'; echo  <- FALSE; fig.show <- 'hide';}
options(tidy=TRUE, width=100)
@

\tableofcontents
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multi-sample variant calling (about 1h30)}

Multi-sample calling has several advantages:
\begin{itemize}
\item Borrow information from well covered samples to identify indels and other small variants, in turn improving the calling for other samples.
\item Uniform calls across samples, which improves sample comparability.
\item Computational improvements associated with the joint management of potentially large collection of samples.
\end{itemize}

Both \texttt{GATK} and \texttt{samtools} provide options for multi-sample calling and we will explore how these scripts are setup today.


\subsection{Installing the reference fasta file (5 minutes)}

As far as I can tell, and unlike \texttt{samtools}, \texttt{GATK} requires that the fasta file matches the sequence dictionary as specified in the header of the BAM file.

Hence a required element for variant calling with human data is the reference sequence in fasta format.
It is also necessary to index this sequence in order to provide a ``dictionary'' of what sequences are present, and how long they are.
This file is large and is therefore not on the \texttt{github} of the class.
It can be downloaded using a \texttt{ftp} site and the \texttt{wget} utility

<<wget.fastq, eval = FALSE, engine = 'bash'>>=
## go to the right folder
cd ../data/
##get the file from the web
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz
##unzip it
gunzip human_g1k_v37.fasta.gz
##index the fasta
samtools faidx human_g1k_v37.fasta
##go back to where you were (or should have been!)
cd ../session4_multi_sample_calling_annotation/
@

These three steps download the compressed reference, unzip it and generate an index for that file.
Please save it in your \texttt{data} folder (at the same level as all the ``session'' directories), because my scripts will assume that this is where this file can be found.

Because \texttt{samtools} can work with smaller reference genome (i.e. no need to have the full genome) I also added a version of chr21 fasta (build 37 of the human genome) in the \texttt{data/reference} folder. 
However, it is zipped to not overwhelm the \texttt{github} site.
To run some scripts you will need to unzip that file using:
<<unzip, engine = 'bash', eval = FALSE>>=
## go to the right folder
cd ../data/reference/
##unzip below
gunzip chr21.fasta.fz
## and back to home base
cd ../../session4_multi_sample_calling_annotation/
@ 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\texttt{Samtools} \texttt{pileup} and \texttt{mpileup} (30 minutes)}
\texttt{samtools} is less feature-rich that \texttt{GATK} but it remains a dependable, solid piece of software that can provide very reliable callsets.
It has been used for large scale projects like UK10K, and there is no reason to doubt the quality of these calls.
Calling samples requires the presence of the reference sequence, in fasta format.
The compressed file \texttt{chr21.fasta} is available on the github site to make it easy.
It is however necessary to uncompress this file using \texttt{gunzip}.

Once this is done, the script \texttt{multi\_sample\_calling\_samtools.sh} describes the steps that generate a VCF file using \texttt{samtools}.

\noindent
{\bf Exercise:} Have a look at the scripts and make sure you can run them (in the simplest case just calling \texttt{sh script\_name.sh} might do). 
They may need minor tweaking to point to the appropriate locations. It is important to understand all the options that are used.


Now we can look at the output and make sure we understand what is there.
For example, using a simple \texttt{awk} script, let us look at position 21:43847096:
<<look.vcf, engine='bash'>>=
awk '{if ( ($2 == "POS") || ($2 == 43847096)) print $1,$2,$4,$5,$9, $10, $15}' \
              results/UBASH31A_samtools.vcf
@ 

Make sure that you understand the meaning of all the fields in the output vcf file.
Of note, you can see the total depth for each sample in the output VCF file, but I do not think that an option exists in \texttt{samtools} to show the depth per allele, unlike GATK (see below).

Another thing that is important is to understand what \texttt{samtools mpileup} actually does, and what format the calls are based on.
I use the command below routinely to make sure that my calls are not buggy, that I have the right sample... that sort of things.
It is usally referred to as MAQ pileup.

<<check.pileup, engine = 'bash'>>=
samtools mpileup -f ../data/reference/chr21.fasta -r 21:43847096-43847096 \
       ../data/BAM_files/HG00250.mapped.ILLUMINA.bwa.GBR.exome.20121211.bam
@ 

In class we will discuss this format and how to interpret this output.  


%%%%%%%%%%%%%%%%%%
\subsection{GATK v2.0 Unified Genotyper (30 minutes)}
The GATK Unified Genotyped (UG) has been the standard approach for GATK for a while, and still delivers high quality and reliable calls.
For reasons that will be outlined in the next subsection, GATK has now transitioned toward a more powerful system called HaplotypeCaller.But there is nothing wrong with still using the UG module of GATK, and this subsection shows how to run such tools in the context of multi-sample calling.

The Unified Genotyper (UG) version of the GATK pipeline is implemented in the script called \texttt{multi\_sample\_calling\_GATK\_UG.sh}.
The key output of this script is the file \texttt{results/GATK\_msamplecalling.vcf}.
Note that GATK sorts the samples by alphabetical order, so the order may not match what was specified in the input BAM list, and it may also not match the VCF output of \texttt{samtools}.
However, one important details about these scripts: for these to run on my computer, I had to specify the path to the GATK java file.
You will need to do the same thing, and replace my path with \texttt{/usr/local/gatk/GenomeAnalysisTK.jar}. Similarly, the path to the fasta file of the human genome may need to be updated. My scripts assume that you save this file in the \texttt{data} folder.

\noindent
{\bf Exercise:} As for the \texttt{samtools} subsection, make sure you can run these scripts, tweak them a bit and understand the options that I used. 
Don't hesitate to look at the \href{http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_genotyper_UnifiedGenotyper.html}{manual page} for the UG tool to help you out.


As before for \texttt{samtools}, we can look at the output VCF file and make sure we understand what is shown:
<<look.vcf.2, engine='bash'>>=
awk '{if ( ($2 == "POS") || ($2 == 43847096)) print $1,$2,$4,$5, $9, $11, $16}' \
            results/GATK_msamplecalling.vcf
@ 

\noindent
{\bf Exercise:} Make sure you fully understand the output from GATK. It differs from GATK in terms of what is shown, but there are also differences in terms of content (look for example at the sequencing depth for HG00255 at this locus). In general, can you make sense of why the read depth would differ? Can you find a GATK update related to this point in the \href{http://gatkforums.broadinstitute.org/discussion/4399/release-notes-for-gatk-version-3-2}{release notes} of the version 3.2?
Observe also that the genotype likelihoods also differ significantly, even with the same input data.


%%%%%%%%%%%%%%%%%%
\subsection{GATK v3.0 \texttt{HaplotypeCaller} version of multi-sample calling (30 minutes)}
The issue with the UG approach to large cohort is, to a large extent, computational.
It is very impractical to call very large cohorts of samples together because of the (n+1) problem: if you add one sample, you ideally would like to not recall the 2,000 other samples at the same time.
How do we go around that? This is the point of the updated GATK pipeline which is well suited for the analysis of very large cohorts.
The principle is simple:
\begin{itemize}
\item Step1: each BAM file is processed individually by the \texttt{HaplotypeCaller} module. The output is a format called gVCF, which extends the concept of the VCF. 
It essentially keeps information at all positions, not only the polymorphic ones. It is essential to separate regions without coverage from the ones with coverage but no polymorphism.
\item Step 2: these gVCF files are combined together using the \texttt{GenotypeGVCF} module of \texttt{GATK} to create a single gVCF file, which can then be processed as usual.
\end{itemize}

An example of the gVCF output is shown below:
<<gVCF.look, engine = 'bash'>>=
awk '{if ( (NR == 199) || (NR == 200) || (NR == 201) ) print $1,$2,$8,$9,$10}' \
       results/HG00257.mapped.ILLUMINA.bwa.GBR.exome.20121211.gvcf
@

\noindent
{\bf Exercise:} These two steps are described in two independent scripts called \texttt{multi\_sample\_calling\_GATK\_HC\_step1.sh} and \texttt{multi\_sample\_calling\_GATK\_HC\_step2.sh}. These scripts will need light editing to run properly, essentially modifying the path to the fasta reference genome file and the path to \texttt{GATK}. Make sure you can run the scripts, and that you understand the options that I used.

\noindent
{\bf Exercise:} In the gVCF output format highlighted above, I show 3 lines.
Can you understand why \texttt{GATK} decides to go to the next line in that manner?
A key tip comes from the command below. Can you see where these options come from in the gVCF file, what they mean, and how this relates to the new lines in the gVCF format?

<<blocks, engine = 'bash'>>=
grep GVCFBlock results/HG00254.mapped.ILLUMINA.bwa.GBR.exome.20121211.gvcf
@ 

<<tips2, tidy = TRUE, echo = echo, results = results>>=
#Note the GQ value that changes from one block to the next at each line of the gVCF file.
##This is what defines the presence of new lines, i.e. the level of compression essentially.
@

\noindent
{\bf Exercise:} An badly intentioned colleague, jealous of your rapid progress with bioinformatics tools, has sabotaged your \texttt{GATK} script by adding two subtle errors that prevent the script from running. 
The script name is  \texttt{multi\_sample\_calling\_GATK\_HC\_step2\_buggy.sh}. Can you understand what these two errors are and debug the script?
<<tips1, tidy = TRUE, echo = echo, results = results>>=
#Error 1: an additional space has been added after a new line sign, which breaks the GATL scripts.
#Error 2: the file containing the gVCF files has been modified and does not have a .list extension anymore, which confuses GATK.
@


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{VCFtools (about 45 minutes)}

VCFtools is a swiss army type tool very handy for VCF manipulations.
It contains a suite of Perl scripts, which we won't use today, as well as a binary executable, which will be the focus of this section.
The documentation for the binary executable is located \href{http://vcftools.sourceforge.net/documentation.html}{here} and \href{http://vcftools.sourceforge.net/man_latest.html}{here}.

The first thing I propose to do is to compare the output of the GATK v2.0 output (Unified Genotyper, or UG) with the output from \texttt{samtools}.


<<vcftools.1, engine ='bash', eval = TRUE, echo = echo, results = results>>=
vcftools --vcf results/UBASH31A_samtools.vcf  --freq --out results/samtools
vcftools --vcf results/GATK_msamplecalling.vcf --freq --out results/GATK_UG
@ 

Note that from this output it is easy to compare the number of calls for both algorithms.
<<wc, engine = 'bash'>>=
wc -l results/samtools.frq results/GATK_UG.frq
@ 

So where does the difference come from? A first thing is the fact that the call to GATK used a minimum quality filter of 30, which was not applied to the \texttt{samtools} output. 
\texttt{VCFtools} allows you to recode VCF files after having applied relevant filters.
Try for example:

<<vcftools2, engine = 'bash'>>=
vcftools --vcf results/UBASH31A_samtools.vcf --minQ 30 --out results/samtools_filtered --recode
@ 

Note the use of the \texttt{recode} argument.
Without this, no new VCF file will be created.
I suggest you try, and you will see nothing but a log file.

\noindent
{\bf Exercise:} Using the VCF output from \texttt{VCFtools}, create a new file with the frequency values for each site.
Is the result more consistent with the output from GATK UG in terms of number of polymorphic sites?
See how you could have created the same frequency file without the intermediate VCF file \texttt{results/samtools\_filtered.recode.vcf}.
Make sure that this ``direct'' frequency file is identical to the one that used the intermediate VCF file.

<<vcftools.answers, engine = 'bash', echo = echo, results = results>>=
vcftools --vcf results/samtools_filtered.recode.vcf --freq --out results/freq_from_intermediate.vcf
vcftools --vcf results/UBASH31A_samtools.vcf --minQ 30 --freq --out results/freq_direct_from_samtools_output.vcf
@


\noindent
{\bf Optional exercise:} For the subset of variants present in both \texttt{samtools} and GATK UG output, generate a graph to compare the estimated variant frequencies (using \texttt{R} most likely, but feel free to use something else if you prefer).

<<plot.pretty, fig.show = fig.show, fig.width = 7, fig.height = 7, results = results, echo = echo, tidy = TRUE>>=
data.samtools <- read.table('results/samtools.frq', skip = 1, col.names = c('CHROM', 'POS', 'N_ALLELES', 'N_CHR', 'All1', 'All2'))
data.gatk <- read.table('results/GATK_UG.frq', skip = 1, col.names = c('CHROM', 'POS', 'N_ALLELES', 'N_CHR', 'All1', 'All2'))

### now I modify the string that contains the allele, to just keep the frequencies
data.samtools$frq.sam <- as.numeric(gsub(pattern = '.*:', data.samtools$All2, replacement = ''))
data.gatk$frq.gatk <- as.numeric(gsub(pattern = '.*:', data.gatk$All2, replacement = ''))

data <- merge(data.samtools, data.gatk, by = 'POS')
plot(x = data$frq.sam, y = data$frq.gatk, xlab = 'Samtools', ylab = 'GATK')
abline(coef(lm(data$frq.gatk ~ data$frq.sam)), col = 'red')
@ 

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variant annotation (about 45 minutes)}

\subsection{Running the variant effect predictor (VEP)}
After having called variants, the next step consists of annotating them.
A widely used and reliable tool is the Variant Effect Predictor (VEP), which is part of the tools provided by Ensembl.
The simplest way to interact with the VEP is the \href{http://www.ensembl.org/Homo_sapiens/Tools/VEP}{web based interface}.
One possible input format, and probably the most widely used, is VCF.
There is no need to copy all the fields, only the first 8 will be sufficient.
For example you could use:

<<prep.VEP, engine = 'bash'>>=
awk '{if ($1 == 21) print $1,$2,$3,$4,$5,$6,$7,$8}' \
            results/UBASH31A_samtools.vcf > results/samtools_for_VEP.vcf
@ 

\noindent
{\bf Exercise:} Note that there is a trap here. As the name indicates, these sequencing reads are supposed to cover the gene {\it UBASH31A}, however, none of the annotated variants is actually located in that gene. Can you figure out what went wrong? Fix it and download the output of the VEP in \texttt{csv} format (small excel like icon on the right side of the blue banner (see Figure \ref{VEP}).

\begin{figure}[!hbt]
  \includegraphics[width=15cm]{fig/screenshot_VEP.png}
  \caption{Small icon on the right side is what you need to download the output in csv format}. \label{VEP}
\end{figure}


Running the VEP directly from the web is a very practical thing to do but it is not effective for very large amount of data (tens of thousand of variants...).
It is possible to run it directly from your computer.
If you do that, the first time you run the VEP, it will download locally all the files that you need.
The installation has been stremalined extensively by the Ensembl team and the \texttt{INSTALL.pl} will fetch and install the minimum set of tools required for the VEP to run.

<<down.VEP, engine = 'bash', eval = FALSE>>=
wget -O VEP.zip https://github.com/Ensembl/ensembl-tools/archive/release/76.zip
unzip VEP.zip
cd ensembl-tools-release-76/scripts/variant_effect_predictor
perl INSTALL.pl
@ 

You can read more about this process at \href{http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html}{this location}.

 
\noindent
{\b Optional Exercise:} Install the VEP locally, including the cached file for the NCBI build 37 of the human genome. Run the VEP against the VCF files that was generated by \texttt{samtools}. Note that this step can be time consuming, and you may hit small technical issues on these computers mostly set up for teaching, but it should be possible to make these scripts work on most linux installs.


%%%%%%%%%%%%
\subsection{Interpreting the output of the VEP}

You should have now downloaded the output of the VEP in csv format, but in case that is not the case my output is available ( \texttt{samtools\_output\_VEP.csv}).
\texttt{R} is probably the best tool to read a csv file like the one generated by the VEP.

\noindent
{\bf Exercise:} Using \texttt{R}, load the output of the VEP. Look at the number of lines and understand the logic of the output, in particular why the number of lines exceeds the number of variants that were provided as input.

%%%%%%%%%%%%%%
\subsection{ANNOVAR as an alternative}

Another software widely used is ANNOVAR. 
It is also written in perl, and you can find all the details \href{http://www.openbioinformatics.org/annovar/}{here}.
In class we will discuss the advantages and weaknessed of ANNOVAR compared to the VEP.
They do deliver broadly comparable results, but the main difference comes from differences in the underlying databases (i.e. GENCODE, Ensembl, UCSC...).

\end{document}
