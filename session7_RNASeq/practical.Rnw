\documentclass[a4paper]{article}
\usepackage[colorlinks=true]{hyperref}
\usepackage[margin=1.5cm,includefoot,footskip=30pt]{geometry}

\title{BCGES short courses, session 7, transcriptome sequencing (RNA-Seq)}
\date{}
\author{Vincent Plagnol}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%knit('practical.Rnw'); system('pdflatex practical')

\maketitle
\tableofcontents

<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'; fig.show <- 'asis'} else {results <- 'hide'; echo  <- FALSE; fig.show <- 'hide';}
options(tidy=TRUE, width=80)
@

\clearpage

\section{GTF format to store information gene-centric information (20 minutes)}

\subsection{Ensembl data}

If one works with genes and exons, it is important to have a format that captures this information.
The file format that does this is the GTF format.
A good place to download GTF file is the \href{Ensembl web page}{http://www.ensembl.org/info/data/ftp/index.html}.
One can start by using the \texttt{curl} function (which is a combination of \texttt{cat} and \texttt{url}) to obtain the first few lines of an example GTF file.

<<gtf.download, engine = 'bash', tidy = TRUE>>=
curl --silent ftp://ftp.ensembl.org/pub/release-76/gtf/homo_sapiens/Homo_sapiens.GRCh38.76.gtf.gz | \
   zcat | head -100 > results/human_gtf_example.gtf
@ 

\noindent
{\bf Exercise:} Go over the GTF format and understand what the fields mean, and how the data are organised.

You can now download the full ensembl file to get an idea of the size of the file.
We will use the wget function that was used before in these practicals (note that the code below is not executed, because too long to go through).
<<wget, engine = 'bash', eval = FALSE>>=
wget -O results/ensembl_human_GRCh38.gtf.gz \
   ftp://ftp.ensembl.org/pub/release-76/gtf/homo_sapiens/Homo_sapiens.GRCh38.76.gtf.gz
@ 


%%%%%%%%%%%%%%%
\subsection{UCSC data}

UCSC is the other obvious place to obtain genome-scale data.
The webpage you want to become familiar with is \href{https://genome.ucsc.edu/cgi-bin/hgTables?command=start}{this one}.

\noindent
{\bf Exercise:} Look for a human GTF file generally equivalent to the one you just downloaded from UCSC.
Compare the sizes of both files, look for differences and similarities.
<<tips1, results = results, echo = echo>>=
#You want to set the group option \textt{mRNA and EST}.
#Use the Human mRNA track
#In output format select \texttt{GTF - gene transfer format}
#Specify a name in the output file
#Maybe request a compressed file to limit transfer time
#Compressed the UCSC GTF is ... and the compressed Ensembl one is 16M.
@ 


%%%%%%%%%%%%%%%%%
\clearpage
\section{Library normalization choices (40 minutes)}

\subsection{A toy example}

A key issue for RNA-Seq data is normalization: how do you compare two RNA-Seq datasets generated at two different time points?
One popular choice is RPKM which stands for reads per kb and per Million reads.
The recipe is simple: divide by the total number of reads, divide by the length of the gene, and multiply by a number (one million, which is what M stands for) to get a number that is not too small.
However, this may not be quite what you want. Indeed, we will know look at an example that illustrates the limitations of this measurement.
Our dataset will be very basic (the code below is \texttt{R} code):

<<example.data>>=
##let us assume that all genes have the same length, to put this problem aside for now
read.data <- data.frame (sample1 = 100, sample2 = c(rep(143, 70), rep(0, 30)))
## so one sample has all genes at level 100
##another has more reads for 70 genes and no read for the other 30
@

{\bf Exercise:} Setting aside the gene length question, how would you compute RPKM values in this case?
To get numbers easier to manipulate, let us rather us the number of reads per 1,000.
How do you look into these RPKM values? Do they match what you would see as the intuitive explanation for these data?

<<RPKM, results = results, echo = echo, tidy = TRUE>>=
RPKM.sample1 <- read.data$sample1 / sum(read.data$sample1) * 1000
RPKM.sample2 <- read.data$sample2 / sum(read.data$sample2) * 1000
table(RPKM.sample1)
table(RPKM.sample2)
## the tables above indicate that for sample1, all genes have the same level of expression (which happens to be 10).
## However for sample 2, 70 genes have a 14, and 30 genes have a 0 value.
## I don't think this matches the intuition we should have in this case.
##It seems to me that in that case one would rather conclude that 30 genes are not expressed in sample2, but that the other genes are pretty much on the same level.
@

So now what to do, and how to interpret the data.
Here is an \href{http://seqanswers.com/forums/showpost.php?p=16468&postcount=13}{interesting forum answer} that recapitulates the problem.
I copy paste below the explanation:

\noindent
To estimate the library size, simply taking the total number of (mapped or unmapped) reads is, in our experience, not a good idea.
Sometimes, a few very strongly expressed genes are differentially expressed, and as they make up a good part of the total counts, they skew this number. After you divide by total counts, these few strongly expressed genes become equal, and the whole rest looks differentially expressed. The following simple alternative works much better:
\begin{itemize}
\item Construct a "reference sample" by taking, for each gene, the geometric mean of the counts in all samples.
\item To get the sequencing depth of a sample relative to the reference, calculate for each gene the quotient of the counts in your sample divided by the counts of the reference sample. Now you have, for each gene, an estimate of the depth ratio.
\item Simply take the median of all the quotients to get the relative depth of the library.
\end{itemize}
This is what the 'estimateSizeFactors' function of our DESeq package does.

This answer summarizes the problem well. One can see that in our case the 30 genes with 0 read have a large effect on the overall expression measurements that is probably not warranted.
We will use the \texttt{DESeq} package to address this issue. 

\noindent
{\bf Exercise:}
There are three functions that you need to estimate the size factors in \texttt{DESeq} based on the dataset above. 
One is \texttt{newCountDataSet} to create a new object that \texttt{DESeq} can manipulate.
The other two functions are: \texttt{estimateSizeFactors} and \texttt{sizeFactors} (the latter extracts the size factors from a \texttt{DESeq} object).
Using these two functions (and starting with loading the \texttt{DESeq} library), can you compute the size factors, normalize the data using these, and get new gene level estimates of expression? Is the result now more consistent with your intutive interpretation of the data?

<<deseq.way, results = results, echo = echo, message = FALSE>>=
### Here is my answer to the question above, in R:
library(DESeq)
CDS <- newCountDataSet(read.data[, 1:2], condition = c('sample1', 'sample2'))
CDS <- estimateSizeFactors(CDS)
size.factors <- sizeFactors(CDS)
print(table(read.data$sample1 / size.factors[ 1 ]))
print(table(read.data$sample2 / size.factors[ 2 ]))
@ 

\subsection{Further reading on normalization of RNA-Seq data}

The issue of normalization of RNA-Seq data has been of interest to a lot of people.
You can for example have a read of Lior Pachter's \href{http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/}{blog post on the matter}. The blog links to a talk that is probably interesting (though I have not yet seen it).

This \href{http://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/}{other blog post} is a good read for the list of available methods, even though I do not think I agree with all the details. In particular the sentence: ``Again, the methods in this section allow for comparison of features with different length WITHIN a sample but not BETWEEN samples'' does not make sense to me. 
If we normalize, it is exactly to compare data across samples (there is no point otherwise).
So while there are caveats, as always, I don't think this (rather crucial) statement makes sense.

%%%%%%%%%%%%%%%%%
\clearpage
\section{Aligning RNA-Seq data and estimating gene expression levels (45 minutes)}

Aligning short-read RNA-Seq data is not fundamentally different from aligning DNA sequencing data.
It is however made more complex by the presence of introns, which can create reads or paired-reads spanning large distances.
A popular aligner for RNA-Seq data is \texttt{tophat} and we will go over some basic commands.

\subsection{Aligning with \texttt{tophat} and \texttt{bowtie}}

It is important to note that the underlying alignment engine for \texttt{tophat} is \texttt{bowtie}, hence many commands are shared with standard calls to \texttt{bowtie}. We start by building a \texttt{bowtie} index for a short portion of chromosome 12, which we will use as an example for this class.
Before you go through these steps, execute the script  \texttt{scripts/tophat\_bowtie\_scripts.sh}.
It will generate all the output files we want to look into, and the following goes through these commands in more details.

<<index, engine = 'bash', eval = FALSE>>=
bowtie2-build -f ../data/RNASeq/chr12_short.fa ../data/RNASeq/chr12_short
@ 

With this, we can now perform the alignment step. But we first create some output folders to store all the output files:
<<mkdir.1, engine = 'bash'>>=
mkdir results/tophat_output_with_gtf
@ 

Now we can start working with the fastq files:
<< fastq, engine = 'bash', eval = FALSE >>=
f1=../data/RNASeq/reads_1.fq.gz
f2=../data/RNASeq/reads_2.fq.gz

tophat --no-coverage-search -o results/tophat_output_with_gtf -r 220 --library-type fr-unstranded \
      --segment-length 30 -G ../data/RNASeq/chr12_short.gtf ../data/RNASeq/chr12_short ${f1} ${f2} \
      --rg-sample test --rg-id test  ## these sets the tags in the header of the BAM file
@

{\bf Exercise: } For now, simply make sure you can run these scripts and that the output makes sense.
Look at the BAM file, make sure you can see the appropriate tags in it. Also look at the general output of tophat.


{\bf Exercise:} Can you see a subtle difference between a RNA-Seq BAM file and a DNA sequencing BAM file?
The read extracted below should illustrate this.
<<look.BAM, engine = 'bash'>>=
samtools index results/tophat_output/accepted_hits.bam
samtools view results/tophat_output/accepted_hits.bam  | grep 16M2150N34M > results/odd_read.sam
@ 

<<answers, echo = echo, results = results>>=
#The Ns in the CIGAR string indicate introns, identified by split reads.
@


%%%%%%%%%%%%%%%
\subsection{Expression level estimation using \texttt{Cufflinks}}

A popular software often associated with \texttt{tophat} is \texttt{cufflinks}.
This piece of software is designed to estimate the abundance of each gene (and potentially isoforms).
A call to \texttt{cufflinks} is pretty straightforward:

<<cufflinks, engine = 'bash', eval = FALSE>>=
cufflinks  -o results/cufflinks_output --GTF ../data/RNASeq/chr12_short.gtf \
   results/tophat_output_with_gtf/accepted_hits.bam
@ 

\noindent
{\bf Exercise:} Go through the output, make sure you understand what all columns mean.
Can you see the distinction between the isoform level and gene based estimates?

%%%%%%%%%%%%%%%
\subsection{What if we do not have a GTF file?}
A quick way to identify introns (and therefore exons) is to use these split reads to see where the gaps in the sequence are.
I propose for example to pick a highly expressed gene ({\it DCP1B}) for example and to look at the introns in that gene.
Because how to do this did not seem obvious to me, I wrote a small perl script that does the parsing (something very basic).
See the example below, and make sure that you can run that code.

<<introns, engine = 'bash'>>=
samtools index results/tophat_output_no_gtf/accepted_hits.bam
samtools view  results/tophat_output_no_gtf/accepted_hits.bam 12:2055213-2113677 | \
  awk '{if ($6 ~ /N/ ) {print;}}' | ./scripts/get_introns.pl | \
  awk '{print $4"_"$5}'| sort | uniq -c > results/DCP1B_no_gtf.tab
@

There are plenty of situations where we do not have a GTF file. It could be because the species is not well annotated, or because for some reason you do not trust a published GTF file (low quality, unusual tissue type...).

{\bf Exercise: } What does the output of \texttt{tophat} look like in the absence of a GTF? Start by creating a folder that will contain the output of tophat and run tophat without a GTF file. Do you identify the same introns and at the same frequency in the gene {\it DCP1B}?  I suggest to start by creating a folder to store the data 
<<mkdir.2, engine = 'bash'>>=
mkdir results/tophat_output_no_gtf
@ 

<<tophat.no.gtf, engine = 'bash', echo = echo, results = results>>=
f1=../data/RNASeq/reads_1.fq.gz
f2=../data/RNASeq/reads_2.fq.gz

tophat --no-coverage-search -o results/tophat_output_no_gtf -r 220 --library-type fr-unstranded \
      --segment-length 30 ../data/RNASeq/chr12_short ${f1} ${f2} \
      --rg-sample test --rg-id test  ## these sets the tags in the header of the BAM file
@ 
  





%%%%%%%%%%%%%%%%%
\clearpage
\section{Differential expression analysis (30 minutes)}

\subsection{Using DESeq}

We start by loading the DESeq package as well as an example dataset from a mouse brain RNA-Seq experiment.
<<<deseq, message = FALSE>>=
library(DESeq)
load("../data/RNASeq/deseq_counts_TDP43.RData")
head(genes.counts)
@ 

We can now define the model for the differential expression analysis:
<<model>>=
formula1 <- count ~ condition
formula0 <- count ~ 1
design.deseq <- c('control', 'control', 'control', 'control', 'KD', 'KD', 'KD', 'KD')
@

And now the computations can properly start.
Note that these steps are very long, and therefore the code is not executed as part of this file (to be more precise, it is executed once, and the output is saved).
<<estimate, eval = FALSE>>=
CDS <- newCountDataSet(genes.counts, condition = design.deseq)

CDS <- estimateSizeFactors(CDS)
CDS <- estimateDispersions(CDS, method = 'pooled')

fit0 <- fitNbinomGLMs( CDS, formula0 )
fit1 <- fitNbinomGLMs( CDS, formula1 )

deseq.pval <- fit1
deseq.pval$EnsemblID <- row.names( deseq.pval)
deseq.pval$basic.pval <- signif(nbinomGLMTest( fit1, fit0 ), 4)
save(list = 'deseq.pval', file = 'results/DE_pvalues_ranked.RData')
@ 


See below some polishing: a multiple testing/false discovery rate Bonferroni-Hochberg analysis, and the ordering of the results by significance of P-values.
<<mtesting>>=
load('results/DE_pvalues_ranked.RData')
deseq.pval$adj.pval <- signif(p.adjust( deseq.pval$basic.pval, method="BH" ), 4)

deseq.pval <- deseq.pval[ order(deseq.pval$basic.pval, decreasing = FALSE), ]
head(deseq.pval)
@


\subsection{Using DESeq2}
Relatively recently, the authors of \texttt{DESeq} have released a new version of the \texttt{DESeq} package and called it \texttt{DESeq2}.
The commands are similar, but there are also differences.
The best way to learn about a \texttt{R} package is to work through the vignette, which highlights the main capabilities of the package.
The vignette for \texttt{DESeq2} is located \href{http://www.bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.pdf}{here}.

\noindent
{\bf Exercise:} Perform a differential expression analysis using the newer \texttt{DESeq2} package. This is essentially about finding the right commands in the vignette and transposing them to your situation.

<<deseq2, echo = echo, results = results, eval = FALSE, message = FALSE>>=
library(DESeq2)
load("../data/RNASeq/deseq_counts_TDP43.RData")
design.deseq <- c('control', 'control', 'control', 'control', 'KD', 'KD', 'KD', 'KD')

dds <- DESeqDataSetFromMatrix(countData = genes.counts,
                              colData = data.frame(KD = design.deseq),
                              design = ~ KD)
dds <- DESeq(dds)
res <- results(dds)
resOrdered <- res[order(res$padj),]
head(resOrdered)

@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Galaxy server}

Have a look at the Galaxy server tools for RNA-Seq analysis.
Can you replicate the alignment steps? And can you run cufflinks as well?
Get a feeling for the tools that Galaxy offers and decide whether you much rather the (more constrained) web interface, or whether you are OK with the command lind tools.
Both solutions are absolutely acceptable.

\end{document}
